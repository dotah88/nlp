{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "def get_data():\n",
    "    with open(\"data/ham_data.txt\", encoding=\"utf8\") as ham_f, open(\"data/spam_data.txt\", encoding=\"utf8\") as spam_f:\n",
    "        ham_data = ham_f.readlines()\n",
    "        spam_data = spam_f.readlines()\n",
    "        \n",
    "\n",
    "        ham_label = np.ones(len(ham_data)).tolist()\n",
    "        spam_label = np.zeros(len(spam_data)).tolist()\n",
    "  \n",
    "\n",
    "        corpus = ham_data + spam_data\n",
    " \n",
    "        labels = ham_label + spam_label\n",
    "\n",
    "    return corpus, labels\n",
    "##corups为两类邮件列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(corpus, labels, test_data_proportion=0.3):\n",
    "\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(corpus, labels,\n",
    "                                                        test_size=test_data_proportion, random_state=42)\n",
    "    return train_X, test_X, train_Y, test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_docs(corpus, labels):\n",
    "    filtered_corpus = []\n",
    "    filtered_labels = []\n",
    "    for doc, label in zip(corpus, labels): \n",
    "        if doc.strip():       ##去掉换行符,非空则加入列表\n",
    "            filtered_corpus.append(doc)\n",
    "            filtered_labels.append(label)\n",
    "\n",
    "    return filtered_corpus, filtered_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    print('准确率:', np.round(\n",
    "        metrics.accuracy_score(true_labels,\n",
    "                               predicted_labels),\n",
    "        2))\n",
    "    print('精度:', np.round(\n",
    "        metrics.precision_score(true_labels,\n",
    "                                predicted_labels,\n",
    "                                average='weighted'),\n",
    "        2))\n",
    "    print('召回率:', np.round(\n",
    "        metrics.recall_score(true_labels,\n",
    "                             predicted_labels,\n",
    "                             average='weighted'),\n",
    "        2))\n",
    "    print('F1得分:', np.round(\n",
    "        metrics.f1_score(true_labels,\n",
    "                         predicted_labels,\n",
    "                         average='weighted'),\n",
    "        2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict_evaluate_model(classifier,\n",
    "                                 train_features, train_labels,\n",
    "                                 test_features, test_labels):\n",
    "    # build model\n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predict using model\n",
    "    predictions = classifier.predict(test_features)\n",
    "    # evaluate model prediction performance\n",
    "    get_metrics(true_labels=test_labels,\n",
    "                predicted_labels=predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import jieba\n",
    "\n",
    "# 加载停用词\n",
    "with open(\"dict/stop_words.utf8\", encoding=\"utf8\") as f:\n",
    "    stopword_list = f.readlines()\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = jieba.cut(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "def normalize_corpus(corpus, tokenize=False):\n",
    "    normalized_corpus = []\n",
    "    for text in corpus:\n",
    "\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_stopwords(text)\n",
    "        normalized_corpus.append(text)\n",
    "        if tokenize:\n",
    "            text = tokenize_text(text)\n",
    "            normalized_corpus.append(text)\n",
    "\n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总的数据量: 10001\n",
      "样本之一: 北京售票员可厉害，嘿嘿，有专座的，会直接拉着脖子指着鼻子让上面的人站起来让 座的，呵呵，比较赞。。。 杭州就是很少有人给让座，除非司机要求乘客那样做。 五一去杭州一个景点玩，车上有两个不到一岁的小孩，就是没有人给让座，没办法家长只能在车上把小孩的推车打开让孩子坐进去，但是孩子还是闹，只能抱着，景点离市区很远，车上很颠，最后家长坐在地上抱孩子，就是没有一个人给让座，要是在北京，一上车就有人让座了\n",
      "\n",
      "样本的label: 1.0\n",
      "实际类型: 正常邮件 垃圾邮件\n",
      "[['10156', '说', '的', '呵呵', '标题', 'Re', '我', '要是', '你', '女朋友', '绝对', '跟', '你', '分手', 'Re', '昨晚', '猫', '又', '闹', '了', '一夜', '嗯', '，', '谁', '说', '我', '养猫', '是因为', '有', '爱心', '我', '跟', '谁', '急', '是', '哈', '，', '不是', '用', '爱心', '区分', '的', '喜欢', '宠物', '的', '就', '养', '，', '不', '喜欢', '的', '就', '不养', '呗', '卷', '卷', '，', '你', '搞', '成', '这', '副', '鬼', '样子', '，', '还', '好意思', '来', '找', '我', '撒娇', '。', '。', '。'], ['中信', '（', '国际', '）', '电子科技', '有限公司', '推出', '新', '产品', '：', '升职', '步步高', '、', '做生意', '发大财', '、', '连', '找', '情人', '都', '用', '的', '上', '，', '详情', '进入', '网址', 'httpwwwusa5588comccc', '电话', '：', '02033770208', '服务', '热线', '：', '013650852999'], ['贵', '公司', '负责人', '：', '你好', '！', '本', '公司', '祥泰', '实业', '有限公司', '）', '具有', '进出口', '及', '国内贸易', '的', '企业', '承', '多家', '公司', '委托', '有', '广告', '建筑工程', '其它', '服务', '商品销售', '等', '的', '发票', '向', '外代', '开', '点数', '优惠', '本', '公司', '原则', '是', '满意', '后', '付款', '有意者', '请来', '电', '洽谈', '电话', '：', '013631690076', '邮箱', '：', 'shitailong8163com', '联系人', '：', '郭生', '如', '给', '贵', '公司', '带来', '不便', '请谅解'], ['李敖来', '大陆', '，', '轰轰烈烈', '，', '热热闹闹', '，', '国内', '有些', '人', '也', '坐不住', '了', '，', '总', '有人', '要', '跳', '出来', '显摆', '显摆', '，', '不过', '，', '没显', '好', '倒', '是', '现', '了', '眼', '。', '大家', '看看', '下面', '的', '材料', '，', '真', '为', '清华', '汗颜', '啊', '！', '资料', '一', '【', '清华大学', '学者', '评李敖', '演讲', '：', '李敖', '老', '矣', '尚能', '骂否', '？', '】', 'httpnewsanhuinewscomsystem20050923001359039shtml'], ['公司', '名称', '：', '北京', '优力', '维尔', '科技', '有限公司', '高科技', '公司', '招聘', '行政助理', '兼', '前台', '一名', '要求', '：', '25', '岁', '以下', '，', '形象', '良好', '，', '有', '责任心', '、', '诚实', '守信', '、', '有', '团队', '意识', '、', '敬业', '，', '可以', '尽快', '就位', '工作', '地点', '：', '中信', '国安', '数码港', '，', '地理位置', '在', '稻香', '园桥', '桥东', '，', '中国航天', '大厦', '西边', '待遇', '面议', '联系方式', '：', '简历', 'mailtowuwei99tsinghuaorgcn', '公司', '的', '新', '网站', '和', 'mail', '服务器', '都', '在建设中', '，', '投递', '的', '简历', '中', '最好', '附带', '照片', '联系电话', '：', '82652066', '工作日', '9001800', '欢迎', '投递', '简历', '，', '合则', '约见'], ['如果', '他', '只是', '想', '找个', '上床', '的', '，', '这招', '可能', '管用', '，', '但', '对', '他', '身边', '的', '女人', '，', '这招', '不', '适用', '，', '聪明', '的', '老板', '不吃', '窝边草', '。', '是否', '只', '上床', '从来不', '由', '男人', '单方面', '说了算', '。', '如果', '是', '的话', '，', '呵呵', '，', '恐怕', '结婚', '率会', '大大降低', '的', '。', '窝边草', '心理', '是', '个', '阻碍', '，', '克服', '它', '就是', '了', '。', '个人', '猜测', '，', '这种', '人', '一般', '喜欢', '能', '给', '他', '安全感', '的', '女人', '，', '即', '传统', '的', '贤妻良母', '。', '至少', '，', '他们', '更', '倾向', '于', '找', '此类', '女人', '做', '老婆', '。', '即使', '他们', '骨子里', '喜欢', '风骚', '的', '女人', '，', '呵呵', '。', '这个', '猜测', '才', '没道理', '。', '你', '在', '毫无道理', '地', '给', '楼主', '泄气', '。'], ['家园网', '提供', '12M', '免费', '主页', '空间', '，', '欢迎', '申请', '家园网', '空间', '特点', '：', '独立', '二级域名', '，', 'Web', '上传', '，', '马上', '申请', '立即', '开通', '，', '永久', '免费', '终生', '稳定', '安全', '无广告', '客服', '在线', '答疑', '。', '网址', 'httpwwwjayacn', '客服', '信箱', 'geduoyeahnet', '网络实名', '：', '家园网', '家园网', '客服', '中心'], ['中信', '（', '国际', '）', '电子科技', '有限公司', '推出', '新', '产品', '：', '升职', '步步高', '、', '做生意', '发大财', '、', '连', '找', '情人', '都', '用', '的', '上', '，', '详情', '进入', '网址', 'httpwwwusa5588comccc', '电话', '：', '02033770208', '服务', '热线', '：', '013650852999']]\n",
      "基于词袋模型特征的贝叶斯分类器\n",
      "准确率: 0.79\n",
      "精度: 0.85\n",
      "召回率: 0.79\n",
      "F1得分: 0.78\n",
      "基于词袋模型特征的逻辑回归\n",
      "准确率: 0.96\n",
      "精度: 0.96\n",
      "召回率: 0.96\n",
      "F1得分: 0.96\n",
      "基于词袋模型的支持向量机\n",
      "准确率: 0.97\n",
      "精度: 0.97\n",
      "召回率: 0.97\n",
      "F1得分: 0.97\n",
      "基于tfidf的贝叶斯模型\n",
      "准确率: 0.79\n",
      "精度: 0.85\n",
      "召回率: 0.79\n",
      "F1得分: 0.78\n",
      "基于tfidf的逻辑回归模型\n",
      "准确率: 0.94\n",
      "精度: 0.94\n",
      "召回率: 0.94\n",
      "F1得分: 0.94\n",
      "基于tfidf的支持向量机模型\n",
      "准确率: 0.97\n",
      "精度: 0.97\n",
      "召回率: 0.97\n",
      "F1得分: 0.97\n",
      "邮件类型: 垃圾邮件\n",
      "预测的邮件类型: 垃圾邮件\n",
      "文本:-\n",
      "中信（国际）电子科技有限公司推出新产品： 升职步步高、做生意发大财、连找情人都用的上，详情进入 网  址:  http://www.usa5588.com/ccc 电话：020-33770208   服务热线：013650852999 \n",
      "邮件类型: 垃圾邮件\n",
      "预测的邮件类型: 垃圾邮件\n",
      "文本:-\n",
      "您好！ 我公司有多余的发票可以向外代开！（国税、地税、运输、广告、海关缴款书）。 如果贵公司（厂）有需要请来电洽谈、咨询！ 联系电话: 013510251389  陈先生 谢谢 顺祝商祺! \n",
      "邮件类型: 垃圾邮件\n",
      "预测的邮件类型: 垃圾邮件\n",
      "文本:-\n",
      "如果您在信箱中不能正常阅读此邮件，请点击这里 \n",
      "邮件类型: 垃圾邮件\n",
      "预测的邮件类型: 垃圾邮件\n",
      "文本:-\n",
      "以下不能正确显示请点此 IFRAME: http://bbs.ewzw.com/viewthread.php?tid=3790 \n",
      "邮件类型: 正常邮件\n",
      "预测的邮件类型: 垃圾邮件\n",
      "文本:-\n",
      "你好，我30，未婚，研究生毕业，想找一个在校的女孩做朋友，每月可以给她2000元零用 我不会走入她的生活中去，为她保密 qq ******* 回了一封信以后： 阿，不愿意跟在一群人后面买单，那会让人觉得特傻 只想一对一交往 【 在 leeann2002 的来信中提到: 】 啊,那您介意找一堆女孩子做朋友吗? 也不用给零用钱拉~~用来带我们一起玩就可以了~,唱歌跳舞什么的~~ (要不2000就把自己卖了...) 有照片连接吗? \n",
      "邮件类型: 正常邮件\n",
      "预测的邮件类型: 垃圾邮件\n",
      "文本:-\n",
      "公司：集制作，创作以及宣传为一体的音乐制作文化发展公司 拥有录音棚，主要制作唱片， 也涉及电影、电视剧、广告等音乐制作及创作 职位：经理助理（说文秘也行） 要求：女   22～25 聪明本分 有一定的协调和办事能力 长相过得去 工作业务范围： 平时管理公司文件 接待、电话、上网 负责安排歌手及制作人的工作下达 待遇：试用期月薪1000 转正1500～2000，另有提成 公司地址：北京市朝阳区麦子店 电话留了会被封么？ 先信箱联系吧 \n",
      "邮件类型: 正常邮件\n",
      "预测的邮件类型: 垃圾邮件\n",
      "文本:-\n",
      "现已确定2006年度不会进行研究生培养机制改革试点，我校研究生招生类别、培养费用 等相关政策仍按现行规定执行，即我校绝大多数研究生属于国家计划内非定向培养生和 定向培养生，培养费由国家财政拨款。少数研究生（主要是除临床医学以外的专业学位 硕士生）属于委托培养生（培养费由选送单位支付）或自筹经费培养生（培养费由考生 本人自筹），其交纳培养费的标准详见我校财务处（网址：http://10.49.99.99）的公 示（2005.9.8）。 \n",
      "邮件类型: 正常邮件\n",
      "预测的邮件类型: 垃圾邮件\n",
      "文本:-\n",
      "提前征友K歌，只要是MM，但是还是有点小要求，age&lt;=23岁，相貌不在乎，身高不限。 K歌地点暂时选在交通大学附近的佰金KTV，因为ME手头有两张优惠券。 那里环境虽然比不上PARTYWORLD，但是比Melody稍微好那么一点点。 如果想报名的人请联系本人。 QQ：275738585 MSN：gao_520@hotmail.com 加我时请务必注明 SMTH 如果有意的MM，请联系我，组织好了大家以后，星期六午饭就在KTV吃。 \n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    corpus, labels = get_data()  # 获取数据集\n",
    "\n",
    "    print(\"总的数据量:\", len(labels))\n",
    "\n",
    "    corpus, labels = remove_empty_docs(corpus, labels)\n",
    "\n",
    "    print('样本之一:', corpus[10])\n",
    "    print('样本的label:', labels[10])\n",
    "    label_name_map = [\"垃圾邮件\", \"正常邮件\"]\n",
    "    print('实际类型:', label_name_map[int(labels[10])], label_name_map[int(labels[5900])])\n",
    "\n",
    "    # 对数据进行划分\n",
    "    train_corpus, test_corpus, train_labels, test_labels = prepare_datasets(corpus,\n",
    "                                                                            labels,\n",
    "                                                                            test_data_proportion=0.3\n",
    "    # 进行归一化\n",
    "    norm_train_corpus = normalize_corpus(train_corpus)\n",
    "    norm_test_corpus = normalize_corpus(test_corpus)\n",
    "    \n",
    "\n",
    "    ''.strip()\n",
    "\n",
    "    from feature_extractors import bow_extractor, tfidf_extractor\n",
    "    import gensim\n",
    "    import jieba\n",
    "\n",
    "    # 词袋模型特征\n",
    "    bow_vectorizer, bow_train_features = bow_extractor(norm_train_corpus)\n",
    "    bow_test_features = bow_vectorizer.transform(norm_test_corpus)\n",
    "\n",
    "    # tfidf 特征\n",
    "    tfidf_vectorizer, tfidf_train_features = tfidf_extractor(norm_train_corpus)\n",
    "    tfidf_test_features = tfidf_vectorizer.transform(norm_test_corpus)\n",
    "\n",
    "    # tokenize documents\n",
    "    tokenized_train = [jieba.lcut(text)\n",
    "                       for text in norm_train_corpus]\n",
    "    print(tokenized_train[2:10])\n",
    "    tokenized_test = [jieba.lcut(text)\n",
    "                      for text in norm_test_corpus]\n",
    "    # build word2vec 模型\n",
    "#     model = gensim.models.Word2Vec(tokenized_train,\n",
    "#                                    size=500,\n",
    "#                                    window=100,\n",
    "#                                    min_count=30,\n",
    "#                                    sample=1e-3)\n",
    "\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    mnb = MultinomialNB()\n",
    "    svm = SGDClassifier(loss='hinge', n_iter=100)\n",
    "    lr = LogisticRegression()\n",
    "\n",
    "    # 基于词袋模型的多项朴素贝叶斯\n",
    "    print(\"基于词袋模型特征的贝叶斯分类器\")\n",
    "    mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                                       train_features=bow_train_features,\n",
    "                                                       train_labels=train_labels,\n",
    "                                                       test_features=bow_test_features,\n",
    "                                                       test_labels=test_labels)\n",
    "\n",
    "    # 基于词袋模型特征的逻辑回归\n",
    "    print(\"基于词袋模型特征的逻辑回归\")\n",
    "    lr_bow_predictions = train_predict_evaluate_model(classifier=lr,\n",
    "                                                      train_features=bow_train_features,\n",
    "                                                      train_labels=train_labels,\n",
    "                                                      test_features=bow_test_features,\n",
    "                                                      test_labels=test_labels)\n",
    "\n",
    "    # 基于词袋模型的支持向量机方法\n",
    "    print(\"基于词袋模型的支持向量机\")\n",
    "    svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                                       train_features=bow_train_features,\n",
    "                                                       train_labels=train_labels,\n",
    "                                                       test_features=bow_test_features,\n",
    "                                                       test_labels=test_labels)\n",
    "\n",
    "\n",
    "    # 基于tfidf的多项式朴素贝叶斯模型\n",
    "    print(\"基于tfidf的贝叶斯模型\")\n",
    "    mnb_tfidf_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                                         train_features=tfidf_train_features,\n",
    "                                                         train_labels=train_labels,\n",
    "                                                         test_features=tfidf_test_features,\n",
    "                                                         test_labels=test_labels)\n",
    "    # 基于tfidf的逻辑回归模型\n",
    "    print(\"基于tfidf的逻辑回归模型\")\n",
    "    lr_tfidf_predictions=train_predict_evaluate_model(classifier=lr,\n",
    "                                                         train_features=tfidf_train_features,\n",
    "                                                         train_labels=train_labels,\n",
    "                                                         test_features=tfidf_test_features,\n",
    "                                                         test_labels=test_labels)\n",
    "\n",
    "\n",
    "    # 基于tfidf的支持向量机模型\n",
    "    print(\"基于tfidf的支持向量机模型\")\n",
    "    svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                                         train_features=tfidf_train_features,\n",
    "                                                         train_labels=train_labels,\n",
    "                                                         test_features=tfidf_test_features,\n",
    "                                                         test_labels=test_labels)\n",
    "\n",
    "\n",
    "\n",
    "    import re\n",
    "\n",
    "    num = 0\n",
    "    for document, label, predicted_label in zip(test_corpus, test_labels, svm_tfidf_predictions):\n",
    "        if label == 0 and predicted_label == 0:\n",
    "            print('邮件类型:', label_name_map[int(label)])\n",
    "            print('预测的邮件类型:', label_name_map[int(predicted_label)])\n",
    "            print('文本:-')\n",
    "            print(re.sub('\\n', ' ', document))\n",
    "\n",
    "            num += 1\n",
    "            if num == 4:\n",
    "                break\n",
    "\n",
    "    num = 0\n",
    "    for document, label, predicted_label in zip(test_corpus, test_labels, svm_tfidf_predictions):\n",
    "        if label == 1 and predicted_label == 0:\n",
    "            print('邮件类型:', label_name_map[int(label)])\n",
    "            print('预测的邮件类型:', label_name_map[int(predicted_label)])\n",
    "            print('文本:-')\n",
    "            print(re.sub('\\n', ' ', document))\n",
    "\n",
    "            num += 1\n",
    "            if num == 4:\n",
    "                break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
